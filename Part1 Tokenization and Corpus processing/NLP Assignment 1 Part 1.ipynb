{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7542661,"sourceType":"datasetVersion","datasetId":4392307},{"sourceId":7555622,"sourceType":"datasetVersion","datasetId":4400405}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nfolder_path = '/kaggle/input/full-contract-txt/full_contract_txt'\n\nwhole_corpus = ''\n\ntext_files = sorted([filename for filename in os.listdir(folder_path) if filename.endswith('.txt')])\n\nfor filename in text_files:\n    if filename.endswith('.txt'):\n        file_path = os.path.join(folder_path, filename)\n        with open(file_path, 'r', encoding='utf-8') as file:\n            whole_corpus += file.read()\n            \noutput_file = 'corpus.txt'\nwith open(output_file, 'w', encoding='utf-8') as file:\n    file.write(whole_corpus)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-04T22:20:38.694741Z","iopub.execute_input":"2024-02-04T22:20:38.695141Z","iopub.status.idle":"2024-02-04T22:20:40.860178Z","shell.execute_reply.started":"2024-02-04T22:20:38.695110Z","shell.execute_reply":"2024-02-04T22:20:40.858974Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\ntokens = word_tokenize(whole_corpus)\noutput_file = 'output.txt'\n\nwith open(output_file, 'w', encoding='utf-8') as file :\n    for token in tokens:\n        file.write(token+'\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:20:40.862322Z","iopub.execute_input":"2024-02-04T22:20:40.862770Z","iopub.status.idle":"2024-02-04T22:21:40.586133Z","shell.execute_reply.started":"2024-02-04T22:20:40.862730Z","shell.execute_reply":"2024-02-04T22:21:40.584941Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"total_tokens = len(tokens)\nprint('Total number of tokens in the corpus: ' ,total_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:21:40.593393Z","iopub.execute_input":"2024-02-04T22:21:40.594223Z","iopub.status.idle":"2024-02-04T22:21:40.602932Z","shell.execute_reply.started":"2024-02-04T22:21:40.594174Z","shell.execute_reply":"2024-02-04T22:21:40.601361Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Total number of tokens in the corpus:  4746517\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_tokens = set(tokens)\nlen_unique_tokens = len(unique_tokens)\nprint('Total number of unique tokens: ', len_unique_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:21:40.604602Z","iopub.execute_input":"2024-02-04T22:21:40.605080Z","iopub.status.idle":"2024-02-04T22:21:41.022456Z","shell.execute_reply.started":"2024-02-04T22:21:40.605035Z","shell.execute_reply":"2024-02-04T22:21:41.021393Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total number of unique tokens:  58850\n","output_type":"stream"}]},{"cell_type":"code","source":"type_token_ratio = len_unique_tokens/total_tokens\nprint('type/token ratio: ', type_token_ratio)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:21:41.023686Z","iopub.execute_input":"2024-02-04T22:21:41.024289Z","iopub.status.idle":"2024-02-04T22:21:41.034529Z","shell.execute_reply.started":"2024-02-04T22:21:41.024255Z","shell.execute_reply":"2024-02-04T22:21:41.033163Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"type/token ratio:  0.0123985650952056\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import Counter\ntoken_counts = Counter(tokens).most_common()\n\noutput_file = 'token.txt'\nwith open(output_file, 'w', encoding='utf-8') as file:\n    for token, count in token_counts:\n        file.write(token + ' ' + str(count) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:21:41.036059Z","iopub.execute_input":"2024-02-04T22:21:41.036568Z","iopub.status.idle":"2024-02-04T22:21:41.868639Z","shell.execute_reply.started":"2024-02-04T22:21:41.036534Z","shell.execute_reply":"2024-02-04T22:21:41.867377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"token_appearnce_once = [token for token, count in token_counts if count == 1]\nprint('Total Number of tokens that appear only once: ', len(token_appearnce_once))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:21:41.870408Z","iopub.execute_input":"2024-02-04T22:21:41.870783Z","iopub.status.idle":"2024-02-04T22:21:41.892055Z","shell.execute_reply.started":"2024-02-04T22:21:41.870751Z","shell.execute_reply":"2024-02-04T22:21:41.890812Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Total Number of tokens that appear only once:  24320\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\ntokens_whole = word_tokenize(whole_corpus)\nprint('Total Number of tokens without any removal : ', len(tokens_whole))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:23:18.512597Z","iopub.execute_input":"2024-02-04T22:23:18.512984Z","iopub.status.idle":"2024-02-04T22:24:14.672173Z","shell.execute_reply.started":"2024-02-04T22:23:18.512955Z","shell.execute_reply":"2024-02-04T22:24:14.671035Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total Number of tokens with any removal :  4746517\n","output_type":"stream"}]},{"cell_type":"code","source":"token_count_whole = Counter(tokens_whole).most_common(20)\n\nfor token, count in token_count_whole:\n    print(token +  ' ' + str(count) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:24:26.579341Z","iopub.execute_input":"2024-02-04T22:24:26.579762Z","iopub.status.idle":"2024-02-04T22:24:27.390550Z","shell.execute_reply.started":"2024-02-04T22:24:26.579730Z","shell.execute_reply":"2024-02-04T22:24:27.388831Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":", 240576\n\nthe 239942\n\nof 151491\n\nto 127052\n\nand 125198\n\n. 117504\n\nor 102674\n\n) 78092\n\n( 75436\n\nin 73940\n\nany 58849\n\n-- 58712\n\nshall 48424\n\na 46205\n\nby 41853\n\nbe 39157\n\nAgreement 37006\n\nfor 35430\n\nthis 35215\n\nsuch 34814\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenization after puncuation removal","metadata":{}},{"cell_type":"code","source":"import re\n\ncorpus_without_punctuation = re.sub(r'[^a-zA-Z\\s]', ' ', whole_corpus)\n\ntokens_without_punctuation = word_tokenize(corpus_without_punctuation)\nprint('Total Number of tokens after removing punctuation: ', len(tokens_without_punctuation))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:51:23.356307Z","iopub.execute_input":"2024-02-04T18:51:23.357962Z","iopub.status.idle":"2024-02-04T18:52:03.948442Z","shell.execute_reply.started":"2024-02-04T18:51:23.357909Z","shell.execute_reply":"2024-02-04T18:52:03.947324Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Total Number of tokens after removing punctuation:  3921917\n","output_type":"stream"}]},{"cell_type":"code","source":"token_count_after_punctuation_removal = Counter(tokens_without_punctuation).most_common(20)\n\nfor token, count in token_count_after_punctuation_removal:\n    print(token +  ' ' + str(count) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:53:39.631261Z","iopub.execute_input":"2024-02-04T18:53:39.631692Z","iopub.status.idle":"2024-02-04T18:53:40.303926Z","shell.execute_reply.started":"2024-02-04T18:53:39.631658Z","shell.execute_reply":"2024-02-04T18:53:40.302760Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"the 239999\n\nof 151825\n\nand 129000\n\nto 127312\n\nor 106445\n\nin 74273\n\nany 58853\n\nshall 48425\n\na 46739\n\nby 42050\n\nbe 39166\n\nAgreement 37036\n\nfor 35481\n\nthis 35217\n\nsuch 34816\n\nwith 32575\n\nas 31639\n\nthat 27281\n\nother 25149\n\nis 21545\n\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_tokens_without_punctuation = set(tokens_without_punctuation)\ntype_token_removal_without_punctuation = len(unique_tokens_without_punctuation)/len(tokens_without_punctuation)\nprint('type/token ration after punctuation removal: ', type_token_removal_without_punctuation)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:53:43.461487Z","iopub.execute_input":"2024-02-04T18:53:43.462472Z","iopub.status.idle":"2024-02-04T18:53:43.687270Z","shell.execute_reply.started":"2024-02-04T18:53:43.462418Z","shell.execute_reply":"2024-02-04T18:53:43.685997Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"type/token ration after punctuation removal:  0.009902045351801173\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenization after puncuation and stopwords removal","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ntokens_without_stopwords = [token for token in tokens_without_punctuation if token.lower() not in stop_words]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:59:38.780482Z","iopub.execute_input":"2024-02-04T18:59:38.780859Z","iopub.status.idle":"2024-02-04T18:59:39.856985Z","shell.execute_reply.started":"2024-02-04T18:59:38.780832Z","shell.execute_reply":"2024-02-04T18:59:39.856122Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"token_count_without_stopwords = Counter(tokens_without_stopwords).most_common(20)\n\nfor token, count in token_count_without_stopwords:\n    print(token +  ' ' + str(count) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:59:41.840757Z","iopub.execute_input":"2024-02-04T18:59:41.842053Z","iopub.status.idle":"2024-02-04T18:59:42.170349Z","shell.execute_reply.started":"2024-02-04T18:59:41.842011Z","shell.execute_reply":"2024-02-04T18:59:42.169211Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"shall 48425\n\nAgreement 37036\n\nParty 20667\n\nmay 13160\n\nSection 12409\n\nparty 11865\n\nCompany 11063\n\nincluding 9588\n\nProduct 8877\n\nuse 8070\n\nprovided 7875\n\nParties 7688\n\ntime 7658\n\nset 6873\n\nwritten 6735\n\napplicable 6477\n\nb 6356\n\ninformation 6342\n\nforth 6238\n\nright 6216\n\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_tokens_without_stopwords = set(tokens_without_stopwords)\ntype_token_ratio_without_stopwrds = len(unique_tokens_without_stopwords)/len(tokens_without_stopwords)\nprint('type/token ration after punctuation and stopword removal: ', type_token_ratio_without_stopwrds)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T19:01:23.096461Z","iopub.execute_input":"2024-02-04T19:01:23.097796Z","iopub.status.idle":"2024-02-04T19:01:23.228441Z","shell.execute_reply.started":"2024-02-04T19:01:23.097749Z","shell.execute_reply":"2024-02-04T19:01:23.227379Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"type/token ration after punctuation and stopword removal:  0.017343033712423427\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk import bigrams\n\nbi_grams = list(bigrams(tokens_without_stopwords))\nbi_gram_counts = Counter(bi_grams).most_common(20)\nfor bi_gram, count in bi_gram_counts:\n    print(str(bi_gram) +  ' ' + str(count) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:00:45.950090Z","iopub.execute_input":"2024-02-04T08:00:45.950602Z","iopub.status.idle":"2024-02-04T08:00:47.826930Z","shell.execute_reply.started":"2024-02-04T08:00:45.950561Z","shell.execute_reply":"2024-02-04T08:00:47.825928Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"('set', 'forth') 6018\n\n('Agreement', 'shall') 3487\n\n('Confidential', 'Information') 2871\n\n('third', 'party') 2427\n\n('written', 'notice') 2370\n\n('Effective', 'Date') 2263\n\n('Party', 'shall') 2228\n\n('Third', 'Party') 2048\n\n('terms', 'conditions') 1902\n\n('prior', 'written') 1807\n\n('forth', 'Section') 1689\n\n('time', 'time') 1684\n\n('shall', 'deemed') 1679\n\n('without', 'limitation') 1658\n\n('Intellectual', 'Property') 1637\n\n('shall', 'mean') 1631\n\n('including', 'without') 1463\n\n('shall', 'provide') 1430\n\n('shall', 'right') 1345\n\n('written', 'consent') 1323\n\n","output_type":"stream"}]}]}